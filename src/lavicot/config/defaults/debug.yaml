# Debug configuration for fast testing
model_name: "Qwen/Qwen2.5-3B-Instruct"
model_type: "lavicot_prefix_attention"  # Options: "lavicot_bias", "lavicot_bias_gamma", "lavicot_prefix_attention"
base_output_dir: "./lavicot/outputs/debug"
resume_checkpoint_path: null

# Minimal training parameters for debugging (calculated for complete pipeline testing)
num_train_epochs: 1
per_device_train_batch_size: 1
per_device_eval_batch_size: 2
gradient_accumulation_steps: 2
learning_rate: 1e-4
max_length: 512  # Minimal sequence length
max_grad_norm: 1.0
warmup_steps: 100  # Minimal warmup

# Very frequent evaluation and saving for debugging
# With 8 train samples, batch size 2, we get 4 training steps
# This ensures we test: train -> eval -> save -> eval -> final_save
logging_steps: 1
save_steps: 3  # Save checkpoint after step 3
eval_steps: 2   # Evaluate after steps 2 and 4
eval_during_training_samples: 2  # Minimal eval samples during training
eval_during_training_fixed: true
generation_temperature: 0.7 # Generation parameters
extract_number_from_answer_only: true

# Dataset configuration - minimal subset
dataset_config_name: "metamath"  # Refers to src/lavicot/config/datasets/gsm8k.yaml


# Prefix tuning configuration
seen_token_weight: 1.0
unseen_token_weight: 0.5
num_rounds: 2
stochastic_rounds: false  # Consistent for debugging
first_round_question_only: true
use_previous_prefix: true
proportion_min: 0.3
proportion_max: 0.7

# Prefix generator configuration - minimal for speed
prefix_generator:
  num_prefix_tokens: 8
  num_blocks: 1
  dropout: 0.01
  use_learnable_queries: true
  layer_selection_mode: "all"
  layer_selection: [0, 1]  # Just first 2 layers for maximum speed
  max_iterations: 3  # Minimal iterations for speed
  gradient_steps: 2  # Minimal gradient steps
  shared_weight_for_all_layers: true  # Share weights for faster training
  use_hooks_during_prefix_update: false

# Minimal evaluation settings for debugging - only test basic functionality
evaluation_settings:
  - setting_index: 1
    reuse_previous_prefix: false
    setting_index_for_prefix_reuse: null
    reuse_previous_cot: false
    setting_index_for_previous_cot: null
    proportion_prev_cot_reuse: null
    iterations: 1

  - setting_index: 2
    reuse_previous_prefix: true
    setting_index_for_prefix_reuse: 1
    reuse_previous_cot: false
    setting_index_for_previous_cot: null
    proportion_prev_cot_reuse: null
    iterations: 5

  - setting_index: 3
    reuse_previous_prefix: true
    setting_index_for_prefix_reuse: 2
    reuse_previous_cot: false
    setting_index_for_previous_cot: null
    proportion_prev_cot_reuse: null
    iterations: 10

  - setting_index: 4
    reuse_previous_prefix: true
    setting_index_for_prefix_reuse: 3
    reuse_previous_cot: true
    setting_index_for_previous_cot: 3
    proportion_prev_cot_reuse: 0.5
    iterations: 10

  - setting_index: 5
    reuse_previous_prefix: true
    setting_index_for_prefix_reuse: 4
    reuse_previous_cot: true
    setting_index_for_previous_cot: 4
    proportion_prev_cot_reuse: 1.0
    iterations: 10

  - setting_index: 6
    reuse_previous_prefix: true
    setting_index_for_prefix_reuse: 5
    reuse_previous_cot: false
    setting_index_for_previous_cot: null
    proportion_prev_cot_reuse: null
    iterations: 10

# Misc
seed: 42

use_wandb: false 
wandb_project: "lavicot-prefix-tuning"
wandb_entity: null
wandb_run_name: null  # Auto-generated if null