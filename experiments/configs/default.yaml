# Model configuration
model_name: "Qwen/Qwen2.5-7B-Instruct"
base_output_dir: "experiments/results"
resume_checkpoint_path: null

# Training hyperparameters
num_train_epochs: 100
per_device_train_batch_size: 4
gradient_accumulation_steps: 8 # effective batch size: 12*4=48
per_device_eval_batch_size: 5
learning_rate: 1e-5
max_length: 512
max_grad_norm: 1.0
warmup_steps: 1000

# Logging and evaluation
logging_steps: 10
save_steps: 5000
eval_steps: 5000
eval_during_training_samples: 10  # Number of samples to use during training evaluation
eval_during_training_fixed: true  # Whether to use fixed samples or random selection each time
generation_temperature: 0.7 # Generation parameters
extract_number_from_answer_only: true

# Dataset configuration
dataset_config_name: "metamath"  # Refers to src/lavicot/config/datasets/gsm8k.yaml

# Prefix tuning configuration
seen_token_weight: 1.0
unseen_token_weight: 1.0
num_rounds: 2  # Number of rounds for training
stochastic_rounds: true  # Whether to use random number of rounds
first_round_question_only: true  # Whether to use only question in first round during training
use_previous_prefix: true  # Whether to use previous round's prefix as initial state
proportion_min: 0.1
proportion_max: 0.9

# Prefix generator configuration
prefix_generator:
  # Layer selection mode: "all", "specific", "exclude", or "spread"
  layer_selection_mode: "all"
  # For "specific" mode: list of layer indices to tune
  # For "exclude" mode: list of layer indices to exclude
  # For "spread" mode: number of layers to spread evenly
  layer_selection: null  # Will be interpreted based on mode

  # Prefix generation configuration
  max_iterations: 7
  gradient_steps: 4  # Last N iterations will have gradients
  shared_weight_for_all_layers: true  # Whether all layers share the same prefix generator
  use_hooks_during_prefix_update: false  # Whether to use hooks during prefix generation

# Evaluation settings
evaluation_settings:
  - setting_index: 1
    reuse_previous_prefix: false
    setting_index_for_prefix_reuse: null
    reuse_previous_cot: false
    setting_index_for_previous_cot: null
    proportion_prev_cot_reuse: null
    iterations: 1

  - setting_index: 2
    reuse_previous_prefix: true
    setting_index_for_prefix_reuse: 1
    reuse_previous_cot: false
    setting_index_for_previous_cot: null
    proportion_prev_cot_reuse: null
    iterations: 5

  - setting_index: 3
    reuse_previous_prefix: true
    setting_index_for_prefix_reuse: 2
    reuse_previous_cot: false
    setting_index_for_previous_cot: null
    proportion_prev_cot_reuse: null
    iterations: 10

  - setting_index: 4
    reuse_previous_prefix: true
    setting_index_for_prefix_reuse: 3
    reuse_previous_cot: true
    setting_index_for_previous_cot: 3
    proportion_prev_cot_reuse: 0.5
    iterations: 10

  - setting_index: 5
    reuse_previous_prefix: true
    setting_index_for_prefix_reuse: 4
    reuse_previous_cot: true
    setting_index_for_previous_cot: 4
    proportion_prev_cot_reuse: 1.0
    iterations: 10

  - setting_index: 6
    reuse_previous_prefix: true
    setting_index_for_prefix_reuse: 5
    reuse_previous_cot: false
    setting_index_for_previous_cot: null
    proportion_prev_cot_reuse: null
    iterations: 10

# Misc
seed: 42

use_wandb: true
# Optional wandb configuration
wandb_project: "lavicot"
wandb_entity: null
wandb_run_name: null

